To make a star have volume when you get close, you must include its data point twice in the speck file.
Without doing this, when the camera gets close, the point dissappears and only the halo is left over.
In addition, the color map color has to be be sufficiently bright (white) and have a large enough alpha.

To have the halo shape instead of texture, make sure datavar for texture is "linked" to the file to overlay

Finding how partiView handles angles was quite complicated. First, the documentation was very brief about it, and only described it
as Rx Ry Rz. The 0 0 0 config of the camera corresponds to facing parallel to the z axis, facnig the origin, with the x axis
extending out to the right and y axis up. 
Testing simple cases, I found that for single rotations about a coordinate axis, it made sense, but not when I composed rotations.
Initially, I thought the system was to first rotate in the x-y plane and then up toward the z axis, similar to latitute longitude, but 
that didn't work in many cases. 

I went to the source code at https://git.ncsa.illinois.edu/slevy/partiview to see if I could find something there. Although partiView itself is 
lightWeight, the source code is still too dense to just "read through". Certain partiView commands like "jump" take in rotation values, so 
I tried to see how it was converted, but there was little code documentation, and I quickly got to hardcoded matrix formulas with no descriptive
names. The file geometry.c seemed to contain most of the utility methods for these operations. However, I did find a comment nearby that described it as "azimuth, elevation, roll", and also YXZ. All along, I suspected that they were using some standard rotation system like Euler Angles, since all other systems require more information (still 3 degrees of freedom, but "spread out", like axis-angle form). 

I found a rotation converter https://www.andre-gaschler.com/rotationconverter/ that I used to convet axis-angle to Euler Angles. Axis-angle
is probably the easiest input data to create, since ultimately I would like to just specify the camera to look in a certain direction.
I use the cross product between that vector and the default camera (0, 0, -1) to find a mutually orthogonal rotation axis, and the dot product
to find the angle swept. 

I also learned that Euler Angles come in many conventions, and 3D rotation in general has many conventions, such as if we use axes fixed in the
body or the frame, or active and passive rotations, etc. However, this converter I found had convention choices permuting XYZ (as opposed to XZX
which is another common convention), so I chose the YXZ that I saw earlier. To test this, I decided to "request" that the camera look along 
<1, 0.5, -1>, and I calculated the axis-angle rotation by hand. I plugged into the converter and got the YXZ Euler Angles, and put them in 
partiView. Luckily, there is a builtin command called "where", which prints out a lot of useful positional information including the camera's 
"looking vector", and it matched my requested <1, 0.5, -1>, up to a very small numerical error. I tested this a couple more times with other
vectors, and it worked correctly as well. 

I also tested orienting myself in partiView randomly, and looking at the camera's direction vector. I again plugged that into axis-angle and then
the converter, and found the calculated Euler Angles. The first 2 matched exactly with what partiView said, but the 3rd angle was about 15% off.
I have no idea why, and I am fairly sure it was not a calculation mistake on my part. 

Now I was pretty confident that the converter was correct, and so I looked at its source code (which was nicely provided in github). Unfortunately,
it was very long and hard to read, and like partiView, the matrix math was all "baked in" and hard to reproduce. 

At this point, I figured out that I would need to convert axis-angle (or some form produceable from axis-angle) to Euler, but most guides
used the standard XYZ (roll pitch yaw) convention. Essentially the ordering of the axes determines the order of the multiplication of the 
elementary rotation matricies. This was a good guide: http://danceswithcode.net/engineeringnotes/rotations_in_3d/rotations_in_3d_part1.html  though
I refered to many other sources. These sources mainly used the composed rotation matrix form, so that's what I used.

The composed rotation matrix I need is created from Ry(v)Rx(u)Rz(w), which of course matches the YXZ form. These matricies are the standard 3D rotations matricies. I used Mathematica to multiply them. To solve for each angle, one can divide carefully chosen elements of the composed matrix
to isolate sin or cos of u, v, w. Just to make sure all my implicit conventions were correct, I testing this against the converter and it all
worked out.   

Finally, to get the rotation matrix from axis-angle I found a formula on wikipedia https://en.wikipedia.org/wiki/Rotation_matrix#In_three_dimensions and used that. There are no annoying conventions here, so it's a
simple plug in. 

I also investigated an alternative method of creating the rotation matrix, using a regular aziumth-elevation type rotation composition. Axis-angle
works well, but it causes some rolling (as in pitch, yaw, roll), and I wanted to see if I could avoid that. Azimuth-elevation is very much like
spherical polar coordinates, and a horizontal section rotated using this method would remain horizontal. I tried this, and it definately produced
different results, but there were some "fun" barrel rolls for some reason, and it was a little disorienting. I decided to stick with axis-angle. 

*Note this is a summary of my path to get the whole thing to work. There were several more dead ends than I mentioned and I also spent some time 
reminding myself how 3D rotations worked in general.


For the camera angles, I have 2 options, looking directly at the line of travel (similar to a rollar coaster), and fixating on a point. Switching between these options produces a smooth pan.

In order to record a video, partiview will save screenshots after each frame in a path. Specifically, load in a flight path manually, then command 'read makeFrames.cf" this instructs partiview to move to frame 1, save a snapshot,
move to frame 2, save, and so on. One can detach the viewing window using "detach" and also resize it using "winsize x y". This enables partiview to take screenshots at any resolution. Unfortunately it saves it directly into the main
directory, not a new folder, but this isn't a huge deal. I then wrote a small python script using opncv to stitch the frames together at 60 fps. 


In order to animate time-varying data, there are several options. One can specify velocity components for each star in the speck file, as a dataavar, and partiview will do a simple 
linear extrapolation in time. This is only good for linear motion. Another option is using the 'datatime' keyword in the speck file. For every star you want to move, you can provide
xyz coordinates for every step in time. There is no interpolation between these points, and so there has to be quite a few datatimes (several hundereds at least) to see smooth
motion. With this method, any sort of path can be produced (for any number of stars concurrently). There is a final method I found with minimal documentation. One can combine the two
previous methods, with linear interpolation between datatimes. However, unless there are many datatimes, this will produce choppy motion, and the velocities have to be updated
appropriately for each data time to make sure the extrapolation hits the next coordinate point. This is cumbersome and one might as well use the "pure datatimes" method. 


In addition to evolving position with time, one can also evolve other qualities with time, as long as they appear in the .SPECK file. This works best with continuous
variables, such as luminosty. All you need to do is to have it print out a non-constant value through time, and partiview will update the quality accordingly.



The only texture I was able to use was the one in the Digital Universe halo.pbm. I tried opening this file and modifying it using GIMP, but after I saved it, partiview had 
trouble recognizing it. It displayed a large square instead of the texture, which is what it does when there are no textures at all. Looking at the raw data using
a text editor, the data format was clearly different after I saved it using GIMP, so that might be the problem.

To fix this, one should use the .sgi format for images, which partiview also supports. When exporting this format from gimp, one can either select the "RLE compression" or
"No compression option". There are several difficulties however within partiview. For one thing, textures are general shown always facing the camera head on. For star halos, this
makes sense because we would not want to see a "flat" star, and the spherically symmetric enough. For showing detail on a planet however, it becomes impossible to oribt the planet,
as the same thing is always shown! There is a way to specify the actual orientation of a texture using the polyorivar command, which accepts 2 orthonormal vectors that define the 
plane of the texture. This is good to show disc galaxies, for example, but again for a planet it will appear flat when viewed edge-on. 

Another difficulity arises when looking at the texture straight on, and that is that it can appear partially transparent. Again, when showing pictures of galaxies with stars "underneath it",
it makes sense to have it be partially transparent. However, planets are not transparent, and it makes for quite an experience when one can see stars through the planet. Changing the 
alpha value on the texture had some effect, though it never made it fully opaque. I experimented with the planet's point color's opaqueness, but that didn't work. One can stack several points
on top of each other, with identical textures, and while that does improve the opaqueness, it never removes it, and the final image appears over-exposed and blurry.

When declaring textures in the .cf file, there are flags to indicate the mode of drawing textures. For example, the standard star halo uses the -M flag, modulate, which treats the texture
as a brightness channel, and respects the point's original color map color. There is another option, -O, over compost, which will obscure features behind it according to the alpha at each
point on the texture. This does succeed in making the planet opaque, but now there are other problems. 

It seems that when partiview says "obscure other features", they really meant that, regardless of depth. When I made a simple earth-sun system, and the sun was physically between the earth 
and the camera, it was actually entirely blocked by the earth texture. In other words, the texture is always the last thing drawn, regardless of depth. Some of the other command flags for 
adding textures did not have this problem, but of course, they made earth transparent. 

Another problem with the -O option, was that it had black "leftovers" around the earth. The image of earth we are interested in is round, but the image file is square, and those 4 extra
pieces showed up. I tried making them transparent in gimp, but then the star's baseline color was shown. Even when I tried to make that color transparent in the color file, it didn't work.
One can mitigate this problem decently by using the polysides command, which controls the baseline texture shape. The default is 4, as that would make a square and is easiest to render. 
The maximum allowed is 16, and this would make a regular 16-sided polygon, which is a more convincing circle than a square. Then, the earth texture is drawn onto this polygon, so there are
now 16 much smaller leftovers. When one makes these parts black, its a pretty convincing effect. One should not zoom too closely if there is a bright object near the edge, as this will
clearly highlight a sharp corner or straight. However, from far away, its decent enough. 

As a final problem, eclipse-like effects cannot be easily produced. This is to be expected, since textures are always drawn at the same luminosity, or at least independant of the 
depth-ordering of bright objects. Therefore when the earth passes in front of the sun, we see a fully illuminated face, as opposed to a silhouette. There is not much that can be
done about this, other than manually changing the texture to a black circle at the right time, and then possibly using a crescent, and eventually the original. 


When showing the orbit path of a planet, I had to use a mesh to create a circle. Partiview does have a ellipsoid command, but this produces an ellipsoid.
One can set one of the dimensions to be 0, but it ends up looking like a wheel with spokes since there are still the longitude lines. One can control the number
of longitude lines, but it wouldn't let me not show any of them. I ended up simply generating a mesh of 150 points in a circle and using that. 

One must also add the planet group after the orbit path group in the master .cf file, or else the orbit path will appear "through" the planet (unless this is
desired).

To achieve a color change effect, possibly for blueshift and redshift, one has to change the color datavar in the .cf file,
which corresponds to an index in the cmap file. However, it seems that when the brightness of a points reaches a certain level,
it will be drawn completely as white. In addition, the intensity of a color somehow "adds" with its base texture. I had the problem
that when changing colors, the radius of the star would change. One has to modify the base texture to have a clear star center and
a distinct halo, so that the center is always nearly white, and the halo is free to change color.

To achieve a dimming effect, specifically when it comes to transits, it is best to use the texture mentioned above. Specifically,
One should make 2 versions, a "dark" and a "light" version. The dark version will be the same as the light except the star halo
will be smaller and less bright (but the star center will be identical). The given star color should remain the same. If one tries
to modify the star color, we have the same problem as before, that the star radius will change noticeably. 

I had also given up on a bug related to stars blinking. When viewing "background stars" every time the camera shifted, certain
stars would "blink into existence" and also blink out, almost randomly. Upon reading the manual closer, I found that this is 
an artifact from having a very close near-clippiing plane. I had set it as 1e-5, with the far-plane at 1e5. This is a very large
ratio that apparently gives the engine a problem; this is a known bug. I set the near-plane to 1e-2, which doesn't cost be anything
since I never view points closer than 0.05, and the blinking stopped!

I also encountered a very annoying bug when it came to mapping colors to stars. This was when I was using data pulled from NASA's
exoplanet archive, and I wanted to highlight different subsets of data within partiview. Specifically, I wanted to divide the data
into planets discovered by Kepler, TESS, K2, and others. Initially, I had each partiview group have the same raw data, and then I
specified an "only=" clause to select the appropriate subset. This worked fine for the color mapping, though there were other problems
when it came to doing a time series animation. It seems that having multiple only= clauses messes something up. I know there are only- and
only+ clauses, but those didn't work either. I then decided that the best way to solve this problem was to seperate the subsets to begin with,
and so I would avoid having the only= clause to select the telescope. For example, the Kepler group would have about 2300 data points,
whereas the TESS file would only have 100 or so. However, I found that the color mapping was wrong, nearly all the stars were always blue,
which was color index 2. I spent a very long time trying to figure out what was wrong and I eventually figured out that it was due
to "residue" color indexes. For basic usage, one might declare 4 colors in the .cmap file, and then use those 4 colors (indexes 0-3)
in the .speck file declaring a color datavar. However, by spliting the data, most files didn't use all of the colors. I was coloring
by detection method (radial velocity, transit, etc.), and when I filtered to only have TESS-found stars, there wasn't going to be any
radial velocity stars. However, the color map file still had indexes for radial velocity. In other words, the color map had indicies 0-3
available, but I only used indicies 1 and 2. Theoretically this is totally fine, but partiview syas otherwise. It appears to somehow try to
reindex everything, and it drops a couple of the first color map indicies. This is why everything was blue, because stars that were meant
to be red defaulted to the only available index. Once I figured this out, I fixed this with an appropriately gimmicky solution, I simply
created dummy stars far away from the areas of interest that used all of the color indicies, so every index had at least 1 star using it.
Essentially, make sure there are no extra "declared" colors (and of course no "request" for non-existant colors). 


One of the most fun/difficult sections of any scene was the TESS sphere animation. What complicated this was the fact that each tile was really a 
"pure" square projected onto the sphere, rather than a square the followed the longitude lines. In other words, the distance between the bottom 2 points
is the same as the top 2 points. This means that the longitude of the upper and lower points are different. I won't go into the fine details here, but you 
can see the code in the scene5/TessProducer.py file. To begin with, I calculated the "master" tile, which was centered on the equator. The latitudes were
easy, but I had to manually compute the longitudes. While there is probably an analytical solution for this, I simply did it quick and dirty, using a 
binary search numerically. However, since the tile angle is 24 degrees, this actually represents a significant arc along the sphere. Therefore, 
I could not use a single square solid mesh to draw this. I divided the tile into 64 smaller tiles, each of which represents a 3x3 degree solid angle, which
is small enough. In addition, I also maintained a list of the edge's coordinates, also subdividing each edge in to 8 segements. 

From here, I used rotation matricies to make copies of all 104 tiles I neede, rotated at the correct angles. At this point, I was back to cartesian coordinates,
so worrying about the convergence of longitude lines wasn't a problem. I first rotated the tiles "up" and then around the polar axis. Finally, we wanted
the final product to be as realsitic as possible, so I had to rotate a final time to align everything with the ecliptic plane/sphere. 

At this point, I have all the raw data to create the animation, but I wanted to do something more fancy than simply fade everything in. I assigned each square
a "starting time", at which point it would begin to draw. Each square takes 60 frames to draw. First, the solid tile itself would fade in during those 60 frames,
and also it would be outlined. The outline took a couple more if statements since the outline is divided into 4 seperate segments. In addition, since therea
are only 4x8 = 32 datapoints for the entire edge, I had to interpolate to find the actual endpoint of the outline. In other words, if I was outlining with linear
progress, I would get a new datapoint roughly every 2 frames, so every other consecutive frame would actually not make any more drawing progress. Therefore I had
to interpolate between consectutive datapoints to find the exact end point. Each square draw itself at its own time, staggered, according to a quadratic function.
This would ensure that the first couple of squares would draw themselves well-separated in time, to emphasize them, and the rest draw themselves faster.

As one might imagine, all this data takes a rather large amount of space. For every single timestep, I need to have meshes for every single tile and all the edges.
Furthermore, I want the final drawn pattern to persist for a while so I can orbit around it for a while. To somewhat mitigate the memory requirement, I isolated
the animation itself, which can't really be compressed, and I created a seperate datagroup "tessSphereStatic" which is time independent, with the full TESS pattern.
This way, once the animation is done, I can immediately turn off the animation group and turn on the static group. This saves quite a bit of space, depending on
how long after you want to show the full pattern.


I also made a secondary animation, that is actually more informational than the first one. This one highlights each "strip" of tiles that TESS actually views at a
time. This was very easy to make however, since I already had the base data. Once simply has to cycle around and highlight each strip using a different color.
I had this data set a new group in partiview, and assigned all color map colors to be yellow, so I don't even need to worry about fade effects.  


At this point, looking at some of the orbit lines, I was rather bothered by the aliasing on them. Specifically, one could clearly see
the "staircase" pattern on these lines especially with parts of the curve that were nearly vertical or horizontal. Partiview does not have
an easy option to antialias this, and initially thought I was stuck with it. However, there are several options to consider. Initially, I
researched some antialiasing methods that I could do given an image. Since I was already using opencv and ffmpeg, I tried to find some
tool that they might have. As can be expected, they didn't have much, since it is much easier to mitigate antialiasing in the production
of an image than after it. Then I ran across supersampling, which sounded promising. Partiview is nice in that it allows you to render videos
in any reasonable resolution (takes proportionally more time of course) and so I started with double width and double height. I then scaled
it back to 1080p using ffmpeg. When I looked at the video, it seemed that it barely helped, and that the orbit lines were now gray instead
of white. I eventually realized that partiview's ability to render at any resolution wasn't quite "honest". When it draws its mesh lines,
it **always** draws them as 1 pixel thick, no matter the frame size. Thus when I tried to rener at higher resolution, the line itself stayed
the same resolution. In addition, the final line appeared gray because I now had 1 white pixel per 3840 instead of 1 white pixel per 1920,
so it averaged to "half a white" per 1920, or in other words, gray. 

I then considered trying to do some simple cv to draw over the lines correctly. I could render the video with only the orbit lines (and
with still the correct motion and angles of the camera), and this would effectively tell opencv where the orbits are with no noise. 
Theoretically opencv could then draw over the lines using properly antialiased lines, and then I could somehow take that information and
put it on top of the full video. However, it seemed pretty difficult and I don't have much experience with this part of opencv, so I decided 
not to try.

However, I took another look at the source code. The best solution would be to modify the source code, recompile it, and use my own version
of partiview. If I could get this to work, then potentially I could fix many other problems with partiview, like that fact that the max
sized polygon has only 16 sides. However, as I took a deeper look at the source code, just to look for where I could try to fix antialiasing,
I was once again reminded how hard it would be. Personally I think the code is more convoluted than it needs to be, and there weren't enough
comments to be helpful. I was hoping to find a "drawLine" method, that I could hopefully pass an antialiasing flag, but I never found such
a thing (it might be there though). I did find a generic draw method which seemed to draw everything, and I could set a property of openGL
at the beginning of the method to enable antialiasing.

The biggest problem now was getting the code to compile. I have a windows computer, and because I have not declared a CS major/minor as of
now, I don't have access to undergrad linux computers. Therefore I had to try and compile everything on windows. partiview does have a limited
guide in its manual to compile itself, and I followed that. I had to get Microsoft Visual Studio for the compiliation of FLTK, and I also
installed cygwin to emulate some linux functionality. It was all going fine until the end. From what I can understand, I needed to run
a ./configure script, which would produce "ready to use" header files and the makefile. This wasn't explicitly mentioned in the windows
compilation steps, but it was in the linux one. I tried to compile it without running ./configure, and I just got a bunch of "file not found"
errors. ./configure scripts have to be run on linux, so I used cygwin for that, but there were some syntax problems. I ran a dos2unix conversion
on the configure file, just to make sure it wasn't the /r/n vs /n problem, but that didn't work. I had already spent the better part of a day
working on this, so I have up on this attempt. 

Luckily, while I was hunting through the source code, I came upon the method that parsed the mesh command, which I used to draw my lines. 
To my amazement, there was hidden flag to control line width: -w. This was not in any of the documentation I had ever read, and I only found it
by looking at the code. Apparently, this feature was added in 2009, so they had 12 years to update the documentation. Nevertheless, I tried
out this flag and it worked. This allowed me to go back to supersampling, because now when I try to increase resolution, I can proportionally
scale up the line thickness, so it is a true upscale. I tried it out and it worked quite nicely. I soon found out that lines weren't the only
things that were pixel-bound; stars' halos and points were also drawn by-pixel instead of by-proportion. This meant that I had to 
increase the ptsize and polysize as well, or else the downscaled star would be basically just a smudge of color. I had to play with 
the numbers a bit to find a good compromise, but I ended up with rendering at 4800x2700 (2.5 times 1080p), with line thickness 3,
and also commanding ptsize 0.05 5, slum 40, and polysize 0.0001. With these modifications, it looks fairly close to the 1080p video, but
its close enough, and most importantly, the antialiasing is much improved. 

I also learned that when resizing to get antialiasing, one must should use ffmpeg instead of opencv. I assume it's a difference in the exaxt interpolation
algorithm, and with more effort, one could probably chose the best one, but the ffmpeg default with the -s switch is good.

One should note that antialiasing is more difficult on some scenes than others. For example, in my scene 5, I make use of a lot of changes in star size
and highlighting effects with stars. I have the slum values fairly well-calibrated for 1080p rendering, but when I go to 2700p, everything is thrown off.
It's also more difficult than multiplying each slum by some constant factor. Luckily, that particular scene needs antialiasing the least, so I actually
didn't do it there at all. If one wanted to however, I would manually adjust all the brightness level to see what looks best. 

When I came back to scene 1 to do the final render, I remembered that I had faked Saturn by being it's own "star". The original reason was that any single
texture that included the rings wold not be circular, and so there would be black areas where no stars showed through around saturn. One could argue that as
long as you didn't come too close to Saturn, this blackout area would be very small relatively, and the missing stars might not be noticeable. This is probably
true, except for when the camera moves, one might see stars (dis)appearing when they got close, but not touchng saturn. The more immediate problem, however,
is that the texture will always face you, including the rings. Having a bland planet texture always face you is not a huge problem, since there is not enough
detail on the surface to indicate orientation, but having the rings always face you (or be at some constant relative orientation) is a clear problem. One
can specify the overall texture's fixed orientation, which will fix the rings in place, but then saturn will appear flat, which is also a problem. 

The solution I came up with was to separate the base saturn texture (generic beige textuer) and the rings texture. The planet texture will always face you,
just like the other planets, but the rings would be set at an orientation. However, a layering problem now arose. Assuming you're looking at some angle to
the rings, you would expect the rings to visible on the "front" side of the planet, and hidden around the back. However, in partiview, that wasn't happening.
If the rings were set to modulate texture brightness, both front and back rings would be hidden by teh planet (which was set to overcomposting to prevent see-through
effects), and if the rings were also overcomposting, then the back rings would also cover the planet! To solve this, I cut the rings in half, and set the "front rings"
texture to overcompost and the back rings to modulate. This is certainly not a perfect solution, since this actually resulted in a slight color discrepancy between
the drawn textures. In addition, if you were looking along a direction to see the border of the rings, you would see the rings suddenly dissappear, even though
they were clearly in your line of sight. However, in scenes 1 and 6, you never look at such an angle, and you are also far enough away that it's a moderatly 
convincing depiction of saturn. Certainly better than a "saturn star" or a saturn with no rings.

I also came back to a long standing problem of the orbit line drawing. For the longest time, whenever I draw a circle using partiview, I noticed that in one
small section, the circle was not fully drawn, it was missing a small arc. In addition, concentric orbits always had the missing arc at the same angular position, no
matter the radius, suggesting a systematic mistake. At first, I thought it was partiview not drawing the line meshes correctly. After all, I still don't fully
understand the mesh command's u and v arguments. I was drawing a circle as actually a 150-sided polygon, which is certainly a vey good approximation in this context,
and so I tried to draw another segment between the last point and the 2nd point. This did fix the problem, however, when I used fade effects, one could clearly see
this segement brighter, because it effectively had twice the alpha. I also tried to draw the entire circle twice over, but this made fading less aesthetic - it would fade less
in the begining and then go out very quickly at the end. Much to my embarrassment, I eventually found out what the problem was. To draw the circle, I was essentially
using polar coordinates to find the xy positions, and I was using 6.28 as 2pi. While this is a decent approximation for some situations, 6.28 is most
definitely not 2pi. Those next decimal places do matter. Once I switched to using python's math.pi, all was well.       

In order to speed up rendering, it is possible to run multiple partiview instances. When I was doing the final render, I had 2 scenes running at once.
I never tried it with more than 2 however, because partiview is sort of sensitive when it comes to memory. Especially when you're rendering at antialiasing 
resolution, and possibly having other programs open, such as the memory-hungry Pycharm, I seen Partiview crash multiple times, so I would not recommend
having more than 2 instances open. When it comes to compressing the images however, ffmpeg is a much sturdier tool, and one can use many concurrent instances.
For example, in order to resize raw images from partiview, one can theoretically have multiple parallel threads running the resizing, since each image is of course
independant. I actually never did this because I don't understand windows shell language, and I couldn't make a simple for loop to select some indicies.
What I did have is a script to apply ffmpeg to all images in a given folder. Theoretically, I could have partiview save half of it's images to 1 folder 
and the other half to another. Since resizing a single frame is roughly as slow as producing it from partiview, thiw would actually save quite a bit of time,
but I never did it.     
  
